## tss 和 pcb

tss 主要是 cpu 提供的东西，pcb 面向的主要是操作系统的东西。

## os 与 cpu

其实在看操作系统，例如下面的进程切换以及内核态用户态切换的时候，经常搞错一些概念。有时候一些看起来相似的概念，实际上面向的对象是不同的，例如上面的 tss 和 pcb，面向的就不同。

还有就是一般我们提 context 一般面向就是进程，task 就是前面的超集合。

https://uniform64.xyz/archives/236

## 调度过程/进程切换/上下文切换

1. 保存处理机现场信息：把 pc 寄存器和 ss，esp 存储到 PCB 中，其实就是存储到 task_struct 的 thread 字段中（linux 的实现）。

2. 按照某种算法取进程：从就绪队列中取一个进程，改变其状态并分配处理机资源 

3. 把处理机分配给进程：按照PCB的信息在处理器相应的寄存器中转入信息，并把处理机控制权赋予进程。

## task_struct 和 pcb

task_struct 就是 linux 中 PCB 的实现，PCB 是一个广泛的概念，操作系统层面上的概念。

## 用户态内核态切换的过程 / 中断的过程/系统调用的过程

https://www.zhihu.com/question/56993503 看浪子天涯的回答  

步骤主要包括：

#### 1、从当前进程的描述符中提取其内核栈的ss0及esp0信息。

*解释*：准确来讲，按照我查到的资料，是从 TSS （任务状态段）获取到 ss0 和 esp0。

什么是 TSS？

TSS 就是任务状态段，你只需要明白本来应该是一个进程一个 TSS ，但是在 linux 的实现当中每个CPU一个 ，里面存放着 ss0 和esp0。

什么是 ss0 和 esp0？

ss0 是 内核态堆栈段，理解上可以理解为 ss0 指向内核栈，而esp0  是栈指针。

#### 2、使用ss0和esp0指向的内核栈将当前进程的cs,eip (pc: cs+epi的实现)，eflags，ss, esp信息保存起来，这个过程也完成了由用户栈找到内核栈的切换过程，同时保存了被暂停执行的程序的下一条指令。

解释一下：eip 和 cs 可以理解成就是pc寄存器的实现，我们通常说的 pc 只是一种概念，这里epi 和 cs 就是具体的实现。

ss 就是用户进程栈，每个进程都有自己的栈，这个在进程空间分布中，esp就是进程栈的栈顶指针。

cs  代码段寄存器，其实我不是很理解这个“段”，跟分段或者进程空间分布中的段是什么关系。

#### 3、将先前由中断向量检索得到的中断处理程序的cs，eip信息装入相应的寄存器，开始执行中断处理程序，这时就转到了内核态的程序执行了。

## 进程切换和系统调用的一点区别

这里不太清楚，但是一些资料上显示，进程切换保存现场的时候，是把数据存储在**PCB（或者讲是用户的栈，总之根据代码可以看出是存储到 task_struct 的 thread 字段里面）**里面的。

而系统调用是存储到**内核栈**上面了。

## 进程和线程的区别

先回答概念，再答区别:

<u>进程：指在系统中正在运行的一个应用程序；程序一旦运行就是进程；进程——资源分配的最小单位。</u>

<u>线程：系统分配处理器时间资源的基本单元，或者说进程之内独立执行的一个单元执行流。线程——程序执行的最小单位</u>。

## 分页

逻辑地址到物理地址的映射过程

1. 提取页号，即逻辑地址最左的n位

2. 以这个页号为索引，查找该进程页表中相应的帧号k

3. 该帧的起始物理地址为k*2^m,被访问字节的物理地址是这个数加上偏移量

![img](https://img-blog.csdn.net/20130430152724381)

举个例子

腾讯今年的实习生笔试的某个选择题

 ![img](https://img-blog.csdn.net/20130430155810249) 

 根据上面的三个步骤可以很容易得到结果：物理地址 = 3 * 8 * 1024 + 9612 % 8192 = 25996 

## 分段

*具体参考资料 <Java源码圈 PDF书籍压缩包>，我百度网盘里面有*

#### 到底什么是分段？

从虚拟地址上来讲，按照分页的思想内存是连续的（虚拟地址连续），物理是不连续的，但是现在我连虚拟地址都是不连续的，但是这个不连续不是指断断续续的，而是划分成一段一段。

即本来虚拟地址可能是1~100，现在我把1~25取出来划分成一段，25~30划分成另外一段...这就是分段。

注意我这里分段最大的特点就是段长不需要统一（1~25和25~30的长度不一样，按照分类去划分）。

逻辑地址到物理地址的映射过程

#### 划分过程

1. 提取段号，即逻辑地址最左的n位s

2. 以这个段号为索引，查找该进程段表中该段的起始物理地址

3. 最右m位表示偏移量，偏移量和段长度比较，如果偏移量大于该长度，则该地址失效

4. 物理地址为该段的起始物理地址加上偏移量的和



![img](https://img-blog.csdn.net/20130430152743662)

## IPC

没多大用，但是看看逻辑，用它来记忆： https://mp.weixin.qq.com/s/5CbYGrylSKx1JwtOiW3aOQ 

有些有用，有些没有 https://www.jianshu.com/p/c1015f5ffa74 

### 管道

在了解管道的本质前，最好先了解以下vfs（虚拟文件系统）

 https://developer.ibm.com/tutorials/l-linux-filesystem/ 

 https://blog.csdn.net/weixin_39278265/article/details/88839027 

#### 本质

实际上管道的设计也是遵循UNIX的“一切皆文件”设计原则的，它本质上就是一个文件（这句话不太合理，本质是内存，实现了文件系统的对象而已，所以算作一个文件系统）。**Linux系统直接把管道实现成了一种文件系统，借助VFS给应用程序提供操作接口**。

虽然实现形态上是文件，但是管道本身并不占用磁盘或者其他外部存储的空间。在Linux的实现上，它占用的是内存空间。所以，**Linux上的管道就是一个操作方式为文件的内存缓冲区**。

#### 实现原理

同其他真正的文件系统（ext3、ext4等）一样，**都实现VFS中的四种主要对象：super_block、inode、dentry和文件对象file** 。

在Linux中，管道是通过指向同一个临时VFS inode的两个file数据结构来实现的，此VFS inode指向内存中的一个物理页面。 

 ![img](http://net.pku.edu.cn/~yhf/lyceum/linuxK/ipc/pipes.gif) 

#### 结论

它确实是一个文件，它实现了文件的定义和各种接口，但是本质上它是以内存的方式存在的，所以它是一种特殊的文件，可以理解为以内存的存在方式实现了文件接口。

一个管道实际上就是个只存在于内存中的文件。

#### 为什么说只有父子能用

因为管道是一种文件系统，在fork的时候父子进程能够共享资源，这里的资源就包括文件操作符fd，所以只有父子能够共享。

#### 注意

管道不能是半双工的，要想进程一边写一边读，必须要两个管道。

### 有名管道

#### 与匿名管道的不同

 FIFO不同于无名管道之处在于它提供了一个路径名与之关联，**以FIFO的文件形式存在于文件系统中**，这样，**即使与FIFO的创建进程不存在亲缘关系的进程，只要可以访问该路径，就能够彼此通过FIFO相互通信** 

#### 读写规则

 http://blog.chinaunix.net/uid-26833883-id-3227144.html 

**记住有名管道读取和写入很麻烦就对了**

#### 创建

管道一般是由一个进程创建的，这里需要提一句，为什么我们说“消息队列独立于进程”，这里特指跟管道的比较，管道从设计上来讲是文件系统（本质是内存），那你是一个文件系统又是一个进程创建的，那么你就算是一个进程的资源，可以就是跟进程绑定在一块了，你当然不能说“管道独立于进程”。

那么父进程就可以对这个资源进行共享，特别是跟自己的子进程共享，这也是匿名管道的关键，有名管道之所以摆脱了这个限制，关键是提供了一个路径，别的进程可以通过这个路径访问这个资源。

### 信号

#### 信号生命周期和处理流程

 （1）信号被某个进程产生，并设置此信号传递的对象（一般为对应进程的pid），然后传递给操作系统；
 （2）操作系统根据接收进程的设置（是否阻塞）而选择性的发送给接收者，如果接收者阻塞该信号（且该信号是可以阻塞的），操作系统将暂时保留该信号，而不传递，直到该进程解除了对此信号的阻塞（如果对应进程已经退出，则丢弃此信号），如果对应进程没有阻塞，操作系统将传递此信号。
 （3）目的进程接收到此信号后，将根据当前进程对此信号设置的预处理方式，暂时终止当前代码的执行，保护上下文（主要包括临时寄存器数据，当前程序位置以及当前CPU的状态）、转而执行中断服务程序，执行完成后在回复到中断的位置。当然，对于抢占式内核，在中断返回时还将引发新的调度。

#### 大白话

基本是这样的，得有个信号注册函数，指名收到某某信号的时候，你的处理函数是什么。

也就是程序接收到对应信号的时候就会执行对应的函数，至于怎么接受的你可以理解为自动接收的，至于信号的产生可以是人为产生也就是ctrl+c，也可以调用一个函数产生。

 https://www.runoob.com/cplusplus/cpp-signal-handling.html 看一下里面signal函数的例子就明白了。

### 消息队列

#### 实现

消息队列是存放在内核中的消息链表，每个消息队列由消息队列标识符表示

#### 与管道的不同

与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列存放在内核中，只有在内核重启(即，操作系统重启)或者显示地删除一个消息队列时，该消息队列才会被真正的删除。

另外与管道不同的是，消息队列在某个进程往一个队列写入消息之前，并不需要另外某个进程在该队列上等待消息的到达。

独立于进程。

### 共享内存

#### 快的原因

因为不需要io，直接读写内存，就像你操作普通变量一样，只不过现在操作的变量在共享的内存里面。

 https://www.cnblogs.com/linuxbug/p/4882776.html 

#### 其他方面需要后序补充

### **信号量** 

#### 本质

可以说本质就是一个非负变量，只不过每次访问这个变量的时候都是原子操作，举个例子，如果信号为1，那么我可以访问临界区，并且把信号量变成0，如果信号量为0，则不能访问临界区。

每次访问临界区之前先访问一下这个信号量。

### 套接字

没啥好说的，用得太多了，tcp/udp

## 函数调用

 https://segmentfault.com/a/1190000007977460 

 https://www.jianshu.com/p/c7154146bdef 

有时间再总结

## 内核栈

 https://blog.csdn.net/yangkuanqaz85988/article/details/52403726 

## 系统调用的过程

1. 用户态程序将描述所需服务的数据存放在特定寄存器中或建立栈帧
2. 用户态程序执行陷阱指令
3. CPU切换成内核态，转跳到指定的特殊指令位置(用户态不能访问)
4. CPU执行这些特殊的指令，根据刚才程序设置的参数执行相应的服务
5. 重置CPU为用户态，返回系统调用结果

## select io 的实现

https://imageslr.github.io/2020/02/27/select-poll-epoll.html#%E4%BB%8E%E9%98%BB%E5%A1%9E-io-%E5%88%B0-io-%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8

这里有源码分析 ：https://www.mscto.com/c/121036.html

#### 过程

1. 拷贝一个 `fd_set` **文件描述符集合**到**内核空间**，注册 **_pollwait** 函数。
2. 遍历 `fd_set`，调用 `fd.poll()` 函数（进来第一次就遍历 fd_set，唤醒后又调用一次，如果一进来就 fd 就绪，那就不需要休眠了）。
3. `fd.poll` （严格来讲是 `poll` 函数里面的  `__pollwait` ）会把当前进程加入**设备等待队列**，最后返回一个**掩码**，**根据掩码判断 fd 是否就绪**。
4. 如果没有 fd 就绪，那就让进程休眠，直到被唤醒，又再一次遍历 **fd_set**，调用 **fd.poll()** 函数。

我们这里补充一下关于唤醒的事情，也就是第 4 点，具体来讲流程是这样的，一般是**网卡**（外部设备）接收到数据，会**产生一个中断信号**，cpu 就会调用对应的**中断处理程序**，这个**中断处理程序就会唤醒进程**。

#### 什么是唤醒进程？

按照这里就是把 socket 等待队列上的进程（本来挂起在 socket 的等待队列上面）移除，加入到操作系统调度的工作队列（就绪队列）。

这里需要注意一下，注册 `_pollwait()`的时候可以注册对应的回调函数的，可以说在 **select 中注册的回调函数就只有唤醒进程这一个作用**，而在 **epoll** 中，**回调函数**除了会**唤醒进程**之外，还会**把 fd 加入就绪队列**，这就是 epoll 和 select 的一大区别之一。

#### 文件描述符集合 `fd_set`

`fd_st` 其实就是一个 1024 长度的 bit 数组（内核代码里面写死的，有类型有长度），例如，我要监听文件描述符1，2，5，那么对应的 fd_set 就是 `...10011`，每一位被置1的说明需要被监听文件描述符。

 使用 `fd_set` 的**二进制每一位**来表示一个文件描述符。

#### 缺点

1. 每次调用 select，都需要**把 fd 集合从用户态拷贝到内核态**，这个开销在fd很多时会很大。
2. 同时每次调用 select 都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大。
3. select支持的文件描述符数量太小了，默认是1024（内核代码中写死的）。
4. 我们的进程需要**遍历** fd_set 以求知道是哪个 fd 有数据。

#### 源码思路

```
// 伪代码
func do_select {
	copy(把文件描述符拷贝进内核)
	
	// 注册poll_wait的回调函数为__pollwait
	// 这样调用 fd.poll 的时候就会调用 _pollwait
	注册 _pollwait()
	
	for {
		// 遍历每个 fd，然后调用 poll
		for fd := range fd_set{
			// 1. 把当前进程加入设备等待队列
			// 2. 返回 mask，根据 mask 判断 fd 是否就绪
			mask = fd.poll
		}
		
		// 根据掩码判断描述符是否就绪
		if mask {
			break
		}
		// 休眠，当被唤醒的时候，又继续遍历 fd，然后调用poll（）函数
		poll_schedule_timeout()
	}

}

func (fd) poll(){
	_pollwait() // 其实就是把当前进程加入到设备等待队列
	...
	return mask
}

func _pollwait() {
	// 设置唤醒进程调用的回调函数，当在驱动中调用 wake_up 唤醒队列时候，
	// pollwake会被调用，这里其实就是调用队列的默认函数 default_wake_function
	// 用来唤醒睡眠的进程。
	init_waitqueue_func_entry(&entry->wait, pollwake); 
	
	add_wait_queue(wait_address, &entry->wait); //加入到等待队列
}
```

## POLL 的实现

跟 select 几乎一毛一样，唯一不一样的地方就是 select 是采用 `fd_set` 来存储文件描述符，而 `fd_set` 是有长度限制的，而 poll 是用**链表**来存储文件描述符。

poll 只解决了长度问题，而开销问题依然没有解决。

## EPOLL 分析实现

https://blog.csdn.net/fanxiushu/article/details/8600826

#### 操作

在讲解原理之前，我们先来看看epoll 平时是怎么操作的，3 个步骤

```
	// 1. 创建 event 事件
	epollfd = epoll_create(FDSIZE);
	
	//2. 添加监听描述符事件（这里如果有多个文件描述符会调用多次）
    epoll_ctl(epollfd,EPOLL_CTL_ADD,fd,&ev);
    
    for ( ; ; )
    {
		//3. 等待文件描述符就绪
 		ret = epoll_wait(epollfd,events,EPOLLEVENTS,-1);
 		handle_events(epollfd,events,ret,listenfd,buf);
     }
```

#### epoll_ctl 的插入

```
// 就是 epoll_ctl 的插入
func ep_insert() {

	// 1. 设置 fd 就绪时的回调函数 ep_epoll_callback
	init_waitqueue_func_entry(&pwq->wait, ep_poll_callback);
	
	// 2. 把新生成关于 epitem 添加到红黑树里
	ep_rbtree_insert(ep, epi); 
}

// 当 fd 就绪时就会调用下面这个函数
func ep_poll_callback(){

	// 1. 把当前就绪的描述epitem结构添加到就绪队列里
    list_add_tail(&epi->rdllink, &ep->rdllist);
    	
    // 2. 如果队列不为空，唤醒 epoll_wait所在进程
    ep_poll_safewake(&ep->poll_wait);
}
```

#### epoll_wait

```
// epoll_wait 内核代码里主要是调用 ep_poll
func ep_poll() {

    // 1. 把当前进程添加到 eventpoll 等待队列
    __add_wait_queue(&ep->wq, &wait);  
    // 如果检测到就绪队列为空，添加当前进程到等待队列，并执行否循环
    for {
    
        // 2. 不断地检查就绪队列是否为空
        if (!list_empty(&ep->rdllist) || !jtimeout)   
        	break;

        // 3. 如果为空就休眠
        schedule_timeout(jtimeout);
	}
}
```

#### 过程

我们根据我们操作 epoll 的几个函数，以及这几个函数的对应伪代码，来讲一讲 epoll 的实现过程

1. epoll_create（待续）
2. epoll_ctl 往 **fd 红黑树**插入新的 fd，还会为 **fd** **就绪时设置回调函数** ep_poll_callback。这个回调函数除了会唤醒进程之外，**还会把 fd 加入就绪队列**。
3. 正式调用 epoll_wait，其实内部就只是**不断地去判断 epoll 就绪队列是否为空**，如果不为空 return，否则休眠。

#### 数据结构

epoll 管理 fd 用的是红黑树，管理就绪的 fd 用的双向链表

#### ET边沿触发和LT水平触发

待续，看下面

#### epoll为什么使用红黑树

因为epoll要求快速找到某个句柄,因此首先是一个Map接口,候选实现:

1. 哈希表 O(1)
2. 红黑树 O(lgn)
3. 跳表 近似O(lgn)
   据说老版本的内核和FreeBSD的Kqueue使用的是哈希表.

个人理解现在内核使用红黑树的原因:

1. 哈希表. 空间因素,可伸缩性.
   (1)频繁增删. 哈希表需要预估空间大小, 这个场景下无法做到.
   间接影响响应时间,假如要resize,原来的数据还得移动.即使用了一致性哈希算法,
   也难以满足非阻塞的timeout时间限制.(时间不稳定)
   (2) 百万级连接,哈希表有镂空的空间,太浪费内存.
2. 跳表. 慢于红黑树. 空间也高.
3. 红黑树. 经验判断,内核的其他地方如防火墙也使用红黑树,实践上看性能最优.

## LT水平触发 和 ET边缘触发

理解上可以通过电信号去记忆。

**高低电平切换瞬间**的触发动作叫边缘触发，**保持电平不变**的触发就叫水平触发。 

#### LT

当文件描述符就绪时，会触发通知，如果用户程序没有一次性把数据读/写完，会一直发通知。

#### ET

仅当描述符从未就绪变为就绪时，通知一次，之后不会再通知。 

#### 为什么边缘触发一定要使用非阻塞 io

> 首先复习一下非阻塞 io，非阻塞 io 在数据准备没有完成的时候，会立马返回。

如果使用 `epoll` 的边缘触发模式，在收到通知时，必须**使用非阻塞 I/O**，**并且必须循环调用 `read` 或 `write` 多次**，直到返回 `EWOULDBLOCK` 为止，然后再调用 `epoll_wait` 等待操作系统的下一次通知。

如果不使用非阻塞 io，也就是阻塞 io的话，那么**一定会在最后一次调用时阻塞**，导致无法正常结束。

使用阻塞 io 的原因就是阻塞io，如果数据准备阶段没有准备好，阻塞 io 会阻塞的，这里就要联系 io 两个阶段：

1. 数据准备阶段。
2. 数据拷贝阶段。

#### select 和 poll

`select` 和 poll 只支持**水平触发**。