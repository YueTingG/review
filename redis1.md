## 数据类型

注意数据类型和数据结构是不同的

**对应数据类型的一些常用命令，然后要搞清楚他们的实现**

| 数据类型 | 可以存储的值           | 操作                                                         |
| -------- | ---------------------- | ------------------------------------------------------------ |
| STRING   | 字符串、整数或者浮点数 | 对整个字符串或者字符串的其中一部分执行操作 对整数和浮点数执行自增或者自减操作 |
| LIST     | 列表                   | 从两端压入或者弹出元素 对单个或者多个元素进行修剪， 只保留一个范围内的元素 |
| SET      | 无序集合               | 添加、获取、移除单个元素 检查一个元素是否存在于集合中 计算交集、并集、差集 从集合里面随机获取元素 |
| HASH     | 包含键值对的无序散列表 | 添加、获取、移除单个键值对 获取所有键值对 检查某个键是否存在 |
| ZSET     | 有序集合               | 添加、获取、删除元素 根据分值范围或者成员来获取元素 计算一个键的排名 |

#### 哈希对象

哈希对象的编码可以是 `ziplist` 或者 `hashtable` 。 

**编码转换**

当哈希对象可以同时满足以下两个条件时， 哈希对象使用 `ziplist` 编码：

1. 哈希对象保存的所有键值对的键和值的字符串长度都小于 `64` 字节；
2. 哈希对象保存的键值对数量小于 `512` 个；

#### 集合对象

 集合对象的编码可以是 `intset` 或者 `hashtable` 。 

**intset**

`intset` 编码的集合对象使用整数集合作为底层实现， 集合对象包含的所有元素都被保存在整数集合里面。 

*其实底层就是数组*

**hashtable**

 `hashtable` 编码的集合对象使用字典作为底层实现， 字典的每个键都是一个字符串对象， 每个字符串对象包含了一个集合元素， 而字典的值则全部被设置为 `NULL` 。 

**编码转换**

当集合对象可以同时满足以下两个条件时， 对象使用 `intset` 编码：

1. 集合对象保存的所有元素都是整数值；
2. 集合对象保存的元素数量不超过 `512` 个；

不能满足这两个条件的集合对象需要使用 `hashtable` 编码。

#### 有序集合对象

 有序集合的编码可以是 `ziplist` 或者 `skiplist` 。 

**skiplist**的实现

主要是map+skiplist，map存储value到score的映射，根据value可以获取score，再根据score去跳跃表上找对应的结点。

**为什么有序集合需要同时使用跳跃表和字典来实现？**

ZRANK，ZRANGE：需要跳跃表来实现

Zscore（获取某个成员的得分）：需要map来实现

在理论上来说， 有序集合可以单独使用字典或者跳跃表的其中一种数据结构来实现， 但无论单独使用字典还是跳跃表， 在性能上对比起同时使用字典和跳跃表都会有所降低。

举个例子， 如果我们只使用字典来实现有序集合， 那么虽然以 O(1) 复杂度查找成员的分值这一特性会被保留， 但是， 因为字典以无序的方式来保存集合元素， 所以每次在执行范围型操作 —— 比如 **ZRANK** 、 **ZRANGE** 等命令时， 程序都需要对字典保存的所有元素进行排序， 完成这种排序需要至少 O(N \log N) 时间复杂度， 以及额外的 O(N) 内存空间 （因为要创建一个数组来保存排序后的元素）。

另一方面， 如果我们只使用跳跃表来实现有序集合， 那么跳跃表执行范围型操作的所有优点都会被保留， 但因为没有了字典， 所以根据成员查找分值这一操作的复杂度将从 O(1) 上升为 O(\log N) 。

因为以上原因， 为了让有序集合的查找和范围型操作都尽可能快地执行， Redis 选择了同时使用字典和跳跃表两种数据结构来实现有序集合。

## 数据结构

 https://redisbook.readthedocs.io/en/latest/compress-datastruct/ziplist.html#ziplist-chapter 

#### 压缩列表

 Ziplist 是由一系列特殊编码的内存块构成的列表 

1. 外面一层构成：**总的字节数+到尾部的偏移量+结点数量+结点们（不是数组，而是合在一起）+末端**
2. 里面每个entry(结点)：**前一个结点的长度+该节点数据类型的编码+该结点数据的长度+该节点的数据**
3. 怎么找到上一个结点，通过前一个结点的长度来找到上一个结点
4. encoding和length决定了保存在content里面的数据和长度

#### 字典

1. 结构
2. rehash
   - 渐进式的rehash，用 `rehashidx`  来记录rehash进行到ht[0]的哪个位置上
   - rehash的触发情况
     - 当结点数大于数组的长度并且 `dict_can_resize`  为真的时候（当进行数据库持久化的时候 `dict_can_resize`  为假），这个时候进行rehash
     - 如果结点数/数组长度大于变量 `dict_force_resize_ratio`  （目前版本是5），强制rehash不管有没有在持久化

#### 跳跃表

资料很不好，明明考的多但是很多人解释得很垃圾

查询：看里面得前进指针 http://redisbook.com/preview/skiplist/datastruct.html 

插入删除： https://juejin.im/post/5dd1169d5188254f6208f962#heading-6 主要要懂得，每个结点的层数是随机取来的，但是很多资料连提都不提，傻逼

span：是用来记录跨度，但是最重要的是能够根据span求出**下一个**结点的排名（不是当前结点的排名，我们是从第一个结点哨兵结点开始遍历的，所以肯定知道第一个结点排名第0，然后第一个结点有span，那么这个span肯定不是指第一节点的排名是多少，不然就是说废话，所以记录的肯定是与下一个结点之间的跨度，那么当前结点加上跨度就可以求出下一个结点的排名）

## 数据淘汰策略

Redis 具体有 6 种淘汰策略：

| 策略            | 描述                                                 |
| --------------- | ---------------------------------------------------- |
| volatile-lru    | 从已设置过期时间的数据集中挑选最近最少使用的数据淘汰 |
| volatile-ttl    | 从已设置过期时间的数据集中挑选将要过期的数据淘汰     |
| volatile-random | 从已设置过期时间的数据集中任意选择数据淘汰           |
| allkeys-lru     | 从所有数据集中挑选最近最少使用的数据淘汰             |
| allkeys-random  | 从所有数据集中任意选择数据进行淘汰                   |
| noeviction      | 禁止驱逐数据                                         |

## redis持久化机制？

详情见 redis设计与实现

**rdb是直接保存数据**，而**aof是保存命令的快照**，需要注意的是，命令中含有数据，所以aof文件比rdb文件大。

#### RDB

1. 主要有`save`和`bgsave`命令，两者都会调用rdbSave函数完成，不同的是save会直接阻塞主进程，二bgsave会fork一个进程，这个进程去完成。
2. 没有专门的载入rdb文件的命令，rdb文件的载入是在服务器启动的时候就完成的事情，所以不需要特别的命令去完成载入这件事，自动完成。
3. 服务器在载入rdb文件的时候会阻塞。
4. save和bgsave不能同时执行，会检测对方是否运行，然后返回错误。

#### AOF

就是当前进程就处理持久化，rdb中的bgsave是子进程处理。

aof可以分3个步骤，命令追加、文件写入、文件同步。

1. **命令追加**。redisServer有个字段是 `aof_buf` 缓冲区，所有的命令都会追加到它的末尾。
2. **文件写入**。redis在执行完文件事件和时间事件之后，会调用flushAppendOnlyFile函数，这个函数会考虑是否将aof_buf缓冲区的内容写入和保存到aof文件里面。这里说明一个问题，并不是文件写入就真的写入了，而是先保存到内核缓冲区，等到缓冲区满了，再真正去写入文件，而这个真正写入文件就叫文件同步，所以才有第3点。
3. **文件同步**。当用户调用write函数的时候，操作系统会把数据保存到内核缓冲区，缓冲区满了才会保存到文件，虽然有效率但是有安全问题，所以操作系统提供了**fsync**和**fdatasync**函数来强制写入文件。

#### AOF重写

是BGREWRITEAOF命令。

会去遍历当前数据库（不是原有的aof文件）所有的key，然后找到这个key对应的值，就可以用一条命令来还原这个值。

是子进程来进行上面说的事情，也就是我们常说的后台进行，不会阻塞主进程。

#### AOF重写不一致问题

问题：aof重写的过程中可能有新的命令到服务器，而重写只能重写之前的。

解决方法：设置**aof重写缓冲区**，注意跟**aof缓冲区**做区别，重写缓冲区主要是用于服务器开启aof重写的时候，此时服务器不仅要把命令发送给aof缓冲区，还要把发送给aof重写缓冲区。

收尾工作：当aof重写工作完成之后，会发送一个信号给服务器，服务器调用信号函数把重写缓冲区的数据加到新的aof文件中，然后对新的文件改名，然后覆盖。

#### 优缺点

- 对于快照持久化，是在某一时刻把所有的数据写入硬盘种，这种方式有可能导致数据不全；在使用这种方式持久化的时候，如果系统发生崩溃，用户会丢失最近一次快照的所有数据，因此使用场景是面对丢失一部分数据也没事的场景；
- 对于AOF持久化，以独立日志的方式记录每次写命令，重启时再重新执行AOF文件中命令达到恢复数据的目的，因此AOF文件的体积将会不断增大，会导致还原数据速度减慢；AOF的主要作用是解决了数据持久化的实时性，目前已经是Redis持久化的主流方式。

## redis是单线程还是多线程？为什么那么快？

单线程

多路复用： https://blog.csdn.net/wx1528159409/article/details/88358997 

多路复用好的原因，因为其他的io模型一次只能监听一个io，我知道你开始误解了，认为这没问题，举例，我服务端监听小明，同时小红想要io，不行办不到，我现在在等小明，小明死活不io小红就不能io，所谓一次只能监听一个io是指你监听了就不能换，不能说小明墨迹就先跟小红搞，不行就不行，只能等对方io完，但是多路复用没这个问题，你一次性监听小明小红，谁好了就搞谁

所以实际上不是多路复用有多好，而是其他的在充当服务端的io的表现太拉跨。

 https://www.cnblogs.com/wajika/p/6581104.html 

#### 快的原因

- 纯内存操作

- - 数据存在内存种

- 单线程操作，避免了频繁的上下文切换

- - 即不存在因为多进程或者多线程导致切换而消耗CPU，也不用去考虑各种锁的问题，不存在加锁释放锁操作；
  - 对于单线程，所有的操作是按照顺序线性执行，但是由于读写操作等待用户输入输出都是阻塞的，所以为了解决这个问题，redis使用I/O多路复用机制；

- 采用了非阻塞 I/O 多路复用机制

- - I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（读就绪或写就绪），能够通知程序进行相应的读写操作。
  - 在I/O多路复用模型种，最重要的是select，该方法可以同时监控多个文件描述符可读可写情况，当其中某些文件描述符可读或者可写时，select方法会返回可读以及可写文件描述符的个数。

## 场景应用

业务实现：https://www.yuque.com/books/share/df1fef8e-748a-4110-936f-84c691c5d7ef/wcv0zs#XDq2r 

一个简单的论坛系统：[https://cyc2018.github.io/CS-Notes/#/notes/Redis?id=%e5%8d%81%e5%9b%9b%e3%80%81%e4%b8%80%e4%b8%aa%e7%ae%80%e5%8d%95%e7%9a%84%e8%ae%ba%e5%9d%9b%e7%b3%bb%e7%bb%9f%e5%88%86%e6%9e%90](https://cyc2018.github.io/CS-Notes/#/notes/Redis?id=十四、一个简单的论坛系统分析) 

## redis的内存回收机制

 Redis的内存回收主要分为过期删除策略和内存淘汰策略两部分。 

 https://juejin.im/post/5d107ad851882576df7fba9e 

 https://www.cnblogs.com/ysocean/p/12422635.html 

需要注意的是，删除key跟淘汰key是不同的两件事，为什么不同，实际上，并不说过期时间到了，key就被删除，除非你用的是定时删除的策略。

事实上然后有可能存在过期了很久但是还没删除的key。

## 缓存穿透

 https://juejin.im/post/5c9a67ac6fb9a070cb24bf34#heading-0 

#### 什么是缓存穿透

正常情况下，我们去查询数据都是存在。

那么请求去查询一条压根儿数据库中根本就不存在的数据，也就是缓存和数据库都查询不到这条数据，但是请求每次都会打到数据库上面去。

这种查询不存在数据的现象我们称为**缓存穿透**。

#### 解决方案

**bloom filter（ 布隆过滤器 ）**：类似于哈希表的一种算法，用所有可能的查询条件生成一个bitmap，在进行数据库查询之前会使用这个bitmap进行过滤，如果不在其中则直接过滤，从而减轻数据库层面的压力。 

**空值缓存**：一种比较简单的解决办法，在第一次查询完不存在的数据后，将该key与对应的空值也放入缓存中，只不过设定为较短的失效时间，例如几分钟，这样则可以应对短时间的大量的该key攻击，设置为较短的失效时间是因为该值可能业务无关，存在意义不大，且该次的查询也未必是攻击者发起，无过久存储的必要，故可以早点失效。

之所以会发生穿透，就是因为缓存中没有存储这些空数据的key。从而导致每次查询都到数据库去了。

那么我们就可以为这些key对应的值设置为null 丢到缓存里面去。后面再出现查询这个key 的请求的时候，直接返回null 。

这样，就不用在到数据库中去走一圈了，但是别忘了设置过期时间。

#### 什么是布隆过滤器

 https://zhuanlan.zhihu.com/p/43263751 

1. 布隆过滤器本质就是一个bit数组，里面一开始都是0。
2. 假设现在有个key，经过某个hash函数映射之后，可以得到k个不同的数字（假设1，3，5）
3. 那么bit数组对应索引（1，3，5）都变成1。
4. 下一次某个key过来的时候，查询的时候用哈希函数的映射结果如果也是1，3，5，那么只能表示这个值**可能**存在。因为你不清楚它跟第一值是否一样（因为没有记录第一值，如果有记录直接比较就完事，但是问题就是记录需要内存或者磁盘来存，如果都存储那就本末倒置了）
5. 但是如果下一次的key映射结果是1，3，4，因为4上面的结果是0，那么一定可以得出这个key不存在，那么这个值就被过滤掉了

结论：布隆过滤器可以判断出某个key**可能存在**以及**一定不存在**。

## 缓存击穿

 https://mp.weixin.qq.com/s/oI3TzbV0UgQpFKYNsAgGzg 

#### 什么是缓存击穿

缓存击穿是我们可能遇到的第二个使用缓存方案可能遇到的问题。

在平常高并发的系统中，大量的请求同时查询一个 key 时，此时这个key正好失效了，就会导致大量的请求都打到数据库上面去。这种现象我们称为**缓存击穿**。

#### 解决方案

1.  一句话，排队，加锁（就看到的代码而言，其实就是全部排队了而不是像有些人说的那样当过期的时候再排队，而是一开始访问这个热门键就排队）

   业界比价普遍的一种做法，即根据key获取value值为空时，锁上，从数据库中`load`数据后再释放锁。若其它线程获取锁失败，则等待一段时间后重试。这里要注意，分布式环境中要使用**分布式锁**，**单机**的话用普通的锁（`synchronized`、`Lock`）就够了。 

   ```
   public String getProduceNum(String key) {
       // 获取分布式锁
       RLock lock = redissonClient.getLock(key);
       try {
           // 获取库存数
           int num= Integer.parseInt(redisTemplate.opsForValue().get(key));  
           // 上锁           
           lock.lock();
           if (num> 0) {
               //减少库存，并存入缓存中
               redisTemplate.opsForValue().set(key, (num - 1) + "");
               System.out.println("剩余库存为num：" + (num- 1));
           } else {
               System.out.println("库存已经为0");
           }
       } catch (NumberFormatException e) {
           e.printStackTrace();
       } finally {
           //解锁
           lock.unlock();
       }
       return "OK";
   }
   ```

2. 不设置过期时间，就是单纯静态数据，下面还有一种，但是我总觉得跟不设置过期时间结果一样

   网上还有另外一种说法是什么，设置在value中设置一个比过期时间还小的过期时间2，然后如果达到了过期时间2（现在还没到真正的过期时间），那么重新从db读取这个值，然后set...我很想问这样不是永远不会过期吗，跟不设置过期时间有什么不同？还更麻烦。

## 缓存雪崩

缓存雪崩 是指在某一个时间段，缓存集中过期失效。此刻无数的请求直接绕开缓存，直接请求数据库。

与缓存击穿最大的不同就是，缓存雪崩式全部过期集中过期，而缓存击穿是热点数据的键过期。

#### 解决方案有以下两种：

1. 搭建高可用的集群，防止单机的redis宕机。
2. 设置不同的过期时间，防止同一时间内大量的key失效。